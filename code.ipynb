{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"maternal_health_risk_data_set.csv\")\n",
    "data.replace({'high risk': 2, 'mid risk':1, 'low risk': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding additional columns if mother is hypertensive or hypotensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBloodPressure(row):\n",
    "    if row['SystolicBP'] > 180 or row['DiastolicBP'] > 120:\n",
    "        return 'hb_crisis'\n",
    "    if row['SystolicBP'] >= 140 or row['DiastolicBP'] >= 90:\n",
    "        return 'hb_crisis_2'\n",
    "    if row['SystolicBP'] >= 130 or row['DiastolicBP'] >= 80:\n",
    "        return 'hb_crisis_1'\n",
    "    if row['SystolicBP'] >= 120 and row['DiastolicBP'] < 80:\n",
    "        return 'elevated'\n",
    "    if row['SystolicBP'] <= 90 or row['DiastolicBP'] <= 60:\n",
    "        return 'hp'\n",
    "    return 'normal'\n",
    "data['BPStatus'] = data.apply (lambda row: checkBloodPressure(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the variable with string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder = LabelEncoder()\n",
    "# data_Y = np.array(label_encoder.fit_transform(data_Y))\n",
    "one_hot = OneHotEncoder(handle_unknown='ignore')\n",
    "data_Y = pd.DataFrame(one_hot.fit_transform(data[['RiskLevel']]).toarray())\n",
    "data_Y = data_Y.iloc[:].values\n",
    "tempdata = pd.DataFrame(one_hot.fit_transform(data[['BPStatus']]).toarray())\n",
    "data = data.join(tempdata)\n",
    "data_X = data.iloc[:,[True,False,False,True,True,True,False,False,True,True,True,True,True]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the datasets into training, testing and validation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "data_X = sc.fit_transform(data_X)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y,\n",
    "    test_size=0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = Sequential()\n",
    "# input layer and 1st hidden layer\n",
    "ann_model.add(Dense(input_dim = 9, units = 128, activation='relu', kernel_initializer='he_uniform'))\n",
    "ann_model.add(Dense(units = 64, activation='relu', kernel_initializer='he_uniform'))\n",
    "ann_model.add(Dense(units = 32, activation='relu', kernel_initializer='he_uniform'))\n",
    "# output layer\n",
    "ann_model.add(Dense(units = 3, activation='sigmoid', kernel_initializer='uniform'))\n",
    "# building the model\n",
    "ann_model.compile(optimizer=Adam(learning_rate=0.01), loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_1 = ann_model.get_weights()\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "df = pd.DataFrame(array_1)\n",
    "df.to_csv('initial_weights.csv')\n",
    "for x in range(5):\n",
    "    print('Run #', x)\n",
    "    ann_model.fit(train_X,train_Y,batch_size=512, epochs=1000, verbose=0)\n",
    "    test_score = ann_model.evaluate(test_X,test_Y, verbose = 0)\n",
    "    train_score = ann_model.evaluate(train_X,train_Y, verbose = 0)\n",
    "    predict_Y = ann_model.predict(test_X).round()\n",
    "    accuracy = accuracy_score(test_Y,predict_Y)\n",
    "    precision = precision_score(test_Y,predict_Y, average='micro')\n",
    "    recall = recall_score(test_Y,predict_Y, average='micro')\n",
    "    f1 = f1_score(test_Y,predict_Y, average='micro')\n",
    "    print(\"Loss and Accuracy of Training Dataset\", train_score)\n",
    "    print(\"Loss and Accuracy of Validation Dataset\", test_score)\n",
    "    print('Accuracy: %.3f' % accuracy)\n",
    "    print('Precision: %.3f' % precision)\n",
    "    print('Recall: %.3f' % recall)\n",
    "    print('F1 Score %.3f' % f1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
